# Offline RAG Evaluation from Golden Questions

Evaluate RAG system performance using "offline" metrics based on historic user conversation threads.

## Overview

This project creates evaluation metrics for the Kunnskapsassistenten RAG system without requiring golden answers. Instead, it evaluates RAG responses against historic user threads using various quality metrics.

**Key characteristics:**
- **Offline evaluation**: Uses historic conversation data, not live queries
- **No golden answers required**: Evaluates based on retrieval quality, relevance, and coherence metrics
- **DeepEval framework**: Built on top of DeepEval for evaluation metrics
- **Input**: Golden questions extracted from production conversations (from `golden_q` project)

## Usage

Run evaluations using:

```bash
uv run python -m src.main ../golden_questions/output/golden_questions.jsonl
```

## Input Format

The project expects golden questions in JSONL format, where each line contains:

```json
{
  "question": "Standalone question text",
  "original_question": "Original user message",
  "conversation_id": "source_conversation_id",
  "context_messages": [],
  "has_retrieval": true,
  "usage_mode": {
    "document_scope": "single_document",
    "operation_type": "simple_qa",
    "output_complexity": "prose"
  },
  "metadata": {
    "topic": "Topic name",
    "user_id": "user_id",
    "created": 1234567890
  }
}
```

## Evaluation Approach

This project evaluates RAG systems using **reference-free metrics**:


### Retrieval evaluations
- **ContextualPrecisionMetric** evaluates whether the reranker in your retriever ranks more relevant nodes in your retrieval context higher than irrelevant ones.
- **ContextualRecallMetric** evaluates whether the embedding model in your retriever is able to accurately capture and retrieve relevant information based on the context of the input.
- **ContextualRelevancyMetric** evaluates whether the text chunk size and top-K of your retriever is able to retrieve information without much irrelevancies.

**Note**: This project does NOT use golden answers. For golden answer-based evaluation, see the separate golden answers project.

## DeepEval Integration

The project uses [DeepEval](https://docs.confident-ai.com/) for:
- Metric definitions and computation
- LLM-based evaluation judges
- Test case management
- Results reporting and visualization

## Dashboard

After running evaluations, visualize results using the interactive Streamlit dashboard.

### How to Run Dashboard

```bash
cd eval_from_golden_questions
streamlit run dashboard/app.py
```

The dashboard will open in your browser at `http://localhost:8501`.

### Dashboard Features

The dashboard provides comprehensive visualization of evaluation results across three main views:

#### 1. Global Metrics View
- **Overview**: Global average scores for each metric type across all test cases
- **Color coding**:
  - Green (score ≥ 0.8) - Good performance
  - Yellow (0.6 ≤ score < 0.8) - Needs improvement
  - Red (score < 0.6) - Critical issues
- **Visualization**: Bar chart showing metric scores
- **Test case count**: Total number of evaluated questions

#### 2. Metric Breakdown
- **Metric selector**: Choose specific metric to analyze (Answer Relevancy, Faithfulness, etc.)
- **Question list**: All questions with scores and success/failure status
- **Details**: Expandable reasons and thresholds for each evaluation
- **Success rate**: Percentage of passing evaluations

#### 3. Individual Question View
- **Full details**: Complete question and answer text
- **All metrics**: Scores, reasons, and thresholds for every metric
- **Retrieved chunks**: Expandable view of RAG context chunks
- **Metadata**: Subject topics and usage mode information

### Multi-Dimensional Filtering

Use sidebar filters to drill down into specific segments:

- **Usage Mode**: Filter by document_scope, operation_type, output_complexity
- **Subject Topics**: Multi-select topics (e.g., "Utdanning og forskning")
- **Combinations**: All filters work together with AND logic

All views update dynamically based on active filters.

### Input Data

The dashboard reads from:
- `output/evaluation_results.jsonl` - Generated by the evaluation pipeline

### Requirements

The dashboard requires additional dependencies (already in pyproject.toml):
- `streamlit` - Web dashboard framework
- `pandas` - Data processing
- `plotly` - Interactive visualizations

These are installed automatically with `uv pip install -e .`

### Dashboard Architecture

```
dashboard/
├── app.py                    # Main Streamlit application
├── data_loader.py            # JSONL parsing and data loading
├── metrics_calc.py           # Metric aggregation and calculations
└── components/               # Reusable UI components
    ├── filters.py            # Multi-dimensional filter UI
    ├── global_view.py        # Global metrics visualization
    └── detail_view.py        # Individual question details
```

### Performance

- Loads and parses evaluation results in < 5 seconds
- UI interactions are responsive (< 500ms for filter updates)
- Handles datasets from small (49 records) to large (700+ records)

For full dashboard specification, see [specs/dashboard_spec.md](specs/dashboard_spec.md).

---

## Project Structure

```
eval_from_golden_questions/
├── README.md                    # This file
├── CLAUDE.md                    # Development guidelines
├── pyproject.toml               # Project dependencies
├── src/                         # Source code
│   ├── main.py                  # Main evaluation script
│   ├── rag_querier.py           # Query RAG system
│   ├── evaluator.py             # Run metrics
│   └── reporter.py              # Generate reports
├── dashboard/                   # Streamlit dashboard
│   ├── app.py                   # Main dashboard application
│   ├── data_loader.py           # Data loading
│   ├── metrics_calc.py          # Metric calculations
│   └── components/              # UI components
├── tests/                       # Unit tests
├── output/                      # Evaluation results (gitignored)
│   └── evaluation_results.jsonl
└── specs/                       # Specifications
    ├── rag_eval_spec.md
    └── dashboard_spec.md
```

## Prerequisites

1. **Python virtual environment**:
   ```bash
   source .venv/bin/activate
   ```

2. **Install dependencies**:
   ```bash
   uv pip install -e .
   ```

3. **Input data**: Golden questions JSONL file from the `golden_questions` project

## Related Projects

- **golden_questions/**: Extracts golden questions from production conversations
- **[Golden answers project]**: Evaluations using manually curated golden answers (separate project)

## Development

This project follows spec-driven TDD development. See [CLAUDE.md](CLAUDE.md) for development guidelines.
